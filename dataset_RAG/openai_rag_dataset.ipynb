{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ace_tools in c:\\users\\hayde\\onedrive - logical aspect\\education\\unisa\\inft3039 - capstone 1\\.venv\\lib\\site-packages (0.0)\n"
     ]
    }
   ],
   "source": [
    "# %pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
    "\n",
    "\n",
    "# !pip install -qU \"langchain[openai]\"\n",
    "# !pip install -qU langchain-pinecone\n",
    "# !pip install -qU langchain-mongodb\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install pyarrow fastparquet\n",
    "# !pip install ace_tools\n",
    "# !pip install langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook produces an inmemory vector database using the Open AI api and langchain openai embeddings.\n",
    "\n",
    "When run for 9500 entries, the Open AI usage calcultor provided a cost of $0.90 a run. \n",
    "\n",
    "The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# import ace_tools as tools\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "import langsmith\n",
    "import getpass\n",
    "import os\n",
    "import json\n",
    "from pydantic.v1 import BaseModel\n",
    "from langchain.chat_models import init_chat_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt', 'essay', 'evaluation', 'band', 'cleaned_evaluation',\n",
       "       'Task Achievement', 'Coherence', 'Lexical Resource', 'Grammar',\n",
       "       'Overall Band Score', 'word_count', 'sentence_count',\n",
       "       'avg_sentence_length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  load csv to dataframe\n",
    "# df = pd.read_csv(r\"C:\\Users\\hayde\\University of South Australia\\USO_Capstone Projects 2025 (SP1 SP3) - Group A - Group A\\Assessment 1\\Training Data\\processed_dataset2_train_data_top100.csv\")\n",
    "# df.head()\n",
    "\n",
    "# Define the GitHub raw CSV URL\n",
    "csv_url = \"https://github.com/haydenkerr/INFT3039-Capstone1-GroupA-25/raw/refs/heads/main/datasets/processed_dataset2_train_data.csv\"\n",
    "# Load the CSV data\n",
    "df = pd.read_csv(csv_url)\n",
    "\n",
    "\n",
    "\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 'prompt', 'essay', 'evaluation', 'band', 'cleaned_evaluation','Task Achievement', 'Coherence', 'Lexical Resource', 'Grammar','Overall Band Score', 'word_count', 'sentence_count','avg_sentence_length'\n",
    "df = df[['prompt', 'essay', 'band', 'cleaned_evaluation','Task Achievement', 'Coherence', 'Lexical Resource', 'Grammar','Overall Band Score']]  \n",
    "\n",
    "df.rename(columns={'prompt':'question'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert each row to a Document object from the dataframe\n",
    "# if rows > 10 , then break for testing\n",
    "docs = []\n",
    "max_rows = 5000\n",
    "processed_rows = 0\n",
    "for _, row in df.iterrows():\n",
    "    if processed_rows >= max_rows:\n",
    "        break\n",
    "    docs.append(\n",
    "        Document(\n",
    "            page_content=f\"question: {row['question']}\\nessay: {row['essay']}\\nband: {row['band']} \\ncleaned_evaluation: {row['cleaned_evaluation']}\\nTask Achievement: {row['Task Achievement']}\\nCoherence: {row['Coherence']}\\nLexical Resource: {row['Lexical Resource']}\\nGrammar: {row['Grammar']}\\nOverall Band Score: {row['Overall Band Score']}\"\n",
    "        )\n",
    "    )\n",
    "    processed_rows += 1\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize In-Memory Vector Store\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "vector_store.add_documents(all_splits)\n",
    "\n",
    "# Load LLM and prompt\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define State for LLM workflow\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    essay: str\n",
    "    context: List[Document]\n",
    "    graded_response: str\n",
    "\n",
    "# Retrieval function\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "# Grading function\n",
    "def grade(state: State):\n",
    "    example_texts = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    user_input = f\"New question: {state['question']}\\nNew Essay: {state['essay']}\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \n",
    "            \"\"\"You are an IELTS examiner. Score the given essay based on the 0-9 IELTS band scale. \n",
    "            The output should be a json object with the following keys:\n",
    "            'question','essay','overall score', 'overall feedback','task achievement score', 'task achievement feedback',\n",
    "            'coherence score', 'coherence feedback', 'lexical resource score', 'lexical resource feedback',\n",
    "            'grammar score', 'grammar feedback'.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Here are some example graded essays:\\n{example_texts}\\n\\nNow, evaluate this new essay:\\n{user_input}\"}\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return {\"graded_response\": response.content}\n",
    "\n",
    "# Build Graph\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, grade])\n",
    "\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'essay', 'band', 'cleaned_evaluation', 'Task Achievement',\n",
       "       'Coherence', 'Lexical Resource', 'Grammar', 'Overall Band Score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test data set\n",
    "# Define the GitHub raw CSV URL\n",
    "\n",
    "\n",
    "# Define the GitHub raw CSV URL\n",
    "csv_url_test = \"https://github.com/haydenkerr/INFT3039-Capstone1-GroupA-25/raw/refs/heads/main/datasets/processed_dataset2_test_data.csv\"\n",
    "# Load the CSV data\n",
    "df_test = pd.read_csv(csv_url_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_test = df_test[['prompt', 'essay', 'band', 'cleaned_evaluation','Task Achievement', 'Coherence', 'Lexical Resource', 'Grammar','Overall Band Score']]  \n",
    "\n",
    "df_test.rename(columns={'prompt':'question'}, inplace=True)\n",
    "\n",
    "df_test.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Children find it difficult to concentrate on or pay attention to school. What are the reasons? How can we solve this problem?\n",
      "Essay: It is true that many children find it difficult to focus on the course at school. There are many reasons behind this phenomenon, so we need to take a comprehensive approach to mitigate it.\n",
      "\n",
      "On the one hand, several factors contribute to this aissue. One of the reasons is the development of technology such as the applications of smartphones. With smartphones, many students are addicted to the virtual world brought about by mobile games and gain a sense of achievement\n",
      "\n",
      ", which cannot be found in schools. The influence of the environment is another main reason. For example, if the juveniles around them behave badly in schools like talking to others or sleeping in classes without being punished by teachers, which may have a bad impact on them and they may imitate this behaviour, leading to their lack of focus on their school work.\n",
      "\n",
      "However, many measures can be taken to solve this problem. To begin with, parents can take children to do some outdoor activities such as kite flying and hiking. They are good ways for juveniles to get close to nature and be away from the virtual world. Then the children may understand the beauty of the real world. Furthermore, improving teaching quality is also important, which means teachers can make their teaching content more abundant to increase the opportunity for interaction with students. For instance, teachers should introduce more entertaining activities in classes such as brainstorming and group discussions. By doing so, the minors can focus more on the course at schools.\n",
      "\n",
      "In conclusion, the development of technology and the influence of the environment are the main reasons why it is difficult for students to catch the course at school, but actions should be taken along several aspects through doing outdoor activities and heightening teaching quality.\n",
      "Overall Score: 5.5\n",
      "---------\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# Example test case\n",
    "question_id = 10\n",
    "# word wrap the text output below  \n",
    "pd.set_option('display.max_colwidth',10 )\n",
    "\n",
    "\n",
    "print(\"Question: \"+df_test['question'][question_id])\n",
    "\n",
    "print(\"Essay: \"+df_test['essay'][question_id])\n",
    "print(\"Overall Score: \"+str(df_test[\"Overall Band Score\"][question_id]))\n",
    "\n",
    "\n",
    "new_question = df_test['question'][question_id]\n",
    "new_essay = df_test['essay'][question_id]\n",
    "print(\"---------\")\n",
    "\n",
    "\n",
    "# Run LLM grading\n",
    "result = graph.invoke({\"question\": new_question, \"essay\": new_essay})\n",
    "# print(result)\n",
    "print(\"---------\")\n",
    "# print(\"Predicted Band Score:\", result[\"Predicted score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result[\"context\"][1].page_content.split(\"\\n\")\n",
    "# convert to json\n",
    "\n",
    "result_json = json.loads(result[\"graded_response\"])\n",
    "result_json[\"overall score\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "---------\n",
      "---------\n",
      "---------\n",
      "---------\n",
      "---------\n",
      "---------\n",
      "---------\n",
      "---------\n",
      "---------\n",
      "---------\n",
      "---------\n",
      "---------\n",
      "---------\n",
      "---------\n",
      "---------\n",
      "---------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m result_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraded_response\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# show accuracy of the model by comparing the predicted score to the actual score\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m new_row \u001b[38;5;241m=\u001b[39m (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m],row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124messay\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;28mfloat\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall Band Score\u001b[39m\u001b[38;5;124m\"\u001b[39m]),result_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall score\u001b[39m\u001b[38;5;124m\"\u001b[39m],\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOverall Band Score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mresult_json\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverall score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     20\u001b[0m rag_results\u001b[38;5;241m.\u001b[39mappend(new_row)  \n\u001b[0;32m     21\u001b[0m processed_rows \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "# for each row in the test data set, run the LLM grading\n",
    "# Run LLM grading for 100 rows\n",
    "rag_results = []\n",
    "processed_rows = 0\n",
    "for _, row in df_test.iterrows():\n",
    "    if processed_rows >= 50:\n",
    "        break\n",
    "    result = graph.invoke({\"question\": row['question'], \"essay\": row['essay']})\n",
    "    # add the results to the rag_results dataframe\n",
    "    # print(\"Question: \"+row['question']+ \"\\n\")\n",
    "    # print(\"Essay: \"+row['essay']+ \"\\n\")\n",
    "    # print(\"Overall Score: \"+str(row[\"Overall Band Score\"])+ \"\\n\")\n",
    "    # print(\"Predicted Band Score:\", result[\"overall score\"])\n",
    "    print(\"---------\")\n",
    "    # add row as list of dictionaries   \n",
    "    result_json = json.loads(result[\"graded_response\"]) \n",
    "    # show accuracy of the model by comparing the predicted score to the actual score\n",
    "    \n",
    "    new_row = (row['question'],row['essay'],float(row[\"Overall Band Score\"]),result_json[\"overall score\"],float(row[\"Overall Band Score\"])/result_json[\"overall score\"])\n",
    "    rag_results.append(new_row)  \n",
    "    processed_rows += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 5)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert list of dictionaries to dataframe\n",
    "rag_results = pd.DataFrame(rag_results, columns = ['question','essay','Overall Band Score','Predicted Band Score','variation'])\n",
    "\n",
    "rag_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rms\n",
      "2.3351927115336757\n",
      "r2Square\n",
      "-1.2969971205265325\n"
     ]
    }
   ],
   "source": [
    "# create accuracy score for continuos variables with sklearn\n",
    "\n",
    "\n",
    "rms = sqrt(mean_squared_error(rag_results['Overall Band Score'], rag_results['Predicted Band Score']))\n",
    "print(\"rms\")\n",
    "print(rms)\n",
    "print(\"r2Square\")\n",
    "y_pred = rag_results['Predicted Band Score']\n",
    "y_true = rag_results['Overall Band Score']\n",
    "openai_r2_score = r2_score(y_true, y_pred)\n",
    "print(openai_r2_score)\n",
    "\n",
    "\n",
    "# save to excel file\n",
    "rag_results.to_excel(\"openai_rag_results.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0rc3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
